{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "How to run: <br> Simply run all the code blocks sequentially."
      ],
      "metadata": {
        "id": "I3JHcmueq0xH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbfBeUQ3q7pX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6405546-4067-47ae-cd7b-76bc8a322ad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#linking Google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install python_mnist\n",
        "\n",
        "!pip install python_mnist"
      ],
      "metadata": {
        "id": "Gg-Q3p32q9Y4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b6118ce-0da5-4e25-bcf5-d7ebf2ffb2a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python_mnist\n",
            "  Downloading python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: python_mnist\n",
            "Successfully installed python_mnist-0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load MINST images\n",
        "\n",
        "from mnist import MNIST\n",
        "import numpy as np\n",
        "\n",
        "mnist_loader = MNIST('/content/drive/My Drive/Colab Notebooks/datasets/MNIST')\n",
        "train_data, train_label = mnist_loader.load_training()\n",
        "test_data, test_label = mnist_loader.load_testing()\n",
        "train_data = np.array(train_data, dtype='float')/255 # norm to [0,1]\n",
        "train_label = np.array(train_label, dtype='short')\n",
        "test_data = np.array(test_data, dtype='float')/255 # norm to [0,1]\n",
        "test_label = np.array(test_label, dtype='short')\n",
        "\n",
        "#add small random noise to avoid matrix singularity\n",
        "train_data += np.random.normal(0,0.0001,train_data.shape) \n",
        "\n",
        "print(train_data.shape, train_label.shape, test_data.shape, test_label.shape)"
      ],
      "metadata": {
        "id": "pkd90NO5rDOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e403bc5a-d80f-417c-cd56-8871cacc5f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784) (60000,) (10000, 784) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model**"
      ],
      "metadata": {
        "id": "rd9OblZ95vPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class mySVM():\n",
        "  def __init__(self, kernel='linear', optimizer='pgd', debug=0, threshold=0.001, \\\n",
        "               lr=1.0, max_epochs=20, batch_size=2, C=1, order=3, gamma=1.0, I=0, J=1):\n",
        "    self.kernel = kernel           # kernel type\n",
        "    self.optimizer = optimizer     # which optimizer is used to solve quadratic programming\n",
        "    self.lr = lr                   # max learning rate in PGD\n",
        "    self.max_epochs = max_epochs   # max epochs in PGD\n",
        "    self.batch_size = batch_size   # size of each subset in PGD\n",
        "    self.debug = debug             # whether print debugging info\n",
        "    self.threshold = threshold     # threshold to filter out support vectors \n",
        "    self.label = {-1:I, 1:J}       # holding the labels\n",
        "\n",
        "    self.C = C                     # C for the soft-margin term\n",
        "    self.order = order             # power order for polynomial kernel\n",
        "    self.gamma = gamma             # gamma for Gaussian RBF kernel\n",
        "\n",
        "  # Kernel Function\n",
        "  # X[N,d]: training samples;  Y[M,d]: other training samples\n",
        "  # return Q[N,N]: linear kernel matrix between X and Y\n",
        "  def Kernel(self, X, Y):\n",
        "    if (self.kernel == 'linear'):\n",
        "      K = X @ Y.T\n",
        "    elif (self.kernel == 'poly'):\n",
        "      K = np.power(X @ Y.T +1, self.order)\n",
        "    elif (self.kernel == 'rbf'): \n",
        "      d1 = np.sum(X*X, axis=1) \n",
        "      d2 = np.sum(Y*Y, axis=1)\n",
        "      K = np.outer(d1, np.ones(Y.shape[0])) + np.outer(np.ones(X.shape[0]), d2) \\\n",
        "          - 2 * X @ Y.T\n",
        "      K = np.exp(-self.gamma * K) \n",
        "      \n",
        "    return K\n",
        "\n",
        "  # construct matrix Q from any kernel function for dual SVM optimization \n",
        "  def QuadraticMatrix(self, X, y):\n",
        "    Q = np.outer(y, y) * self.Kernel(X, X) \n",
        "    return Q\n",
        "\n",
        "      ###############################################################################################################################\n",
        "      ################################################### MODIFIED CODE BELOW #######################################################\n",
        "      ###############################################################################################################################\n",
        "\n",
        "  # clipping function\n",
        "  def clip(self, a, L, H):\n",
        "    a_new = max(L, a)\n",
        "    a_new = min(H, a_new)\n",
        "    return a_new\n",
        "\n",
        "  # use SMO to update 2 alphas in each iteration\n",
        "  def SMO(self, Q, X, y):\n",
        "    N = Q.shape[0]   # num of training samples\n",
        "    alpha = np.zeros(N)\n",
        "    epoch = 0\n",
        "    while epoch < self.max_epochs:\n",
        "      i = 0\n",
        "      #pick i and j = i + 1\n",
        "      while i < N:\n",
        "        if i == N - 1:\n",
        "          j = 0\n",
        "        else:\n",
        "          j = i + 1\n",
        "\n",
        "        #update alpha i and j (based on my attempt of assignment 2 question 3; the formula given in my report)\n",
        "        i_new = (1 - y[i]*y[j] - 2*Q[i][j]*(y[i]*y[j]*alpha[i] + alpha[j]) + 2*Q[j][j]*(alpha[i] + y[i]*y[j]*alpha[j]))/(2*(Q[i][i] - 2*Q[i][j]*y[i]*y[j] + Q[j][j]))\n",
        "        if y[i] != y[j]:\n",
        "          i_new = self.clip(i_new, max(0, alpha[j] - alpha[i]), min(self.C, self.C + alpha[j] - alpha[i]))\n",
        "        else:\n",
        "          i_new = self.clip(i_new, max(0, alpha[j] + alpha[i] - self.C), min(self.C, alpha[j] + alpha[i]))\n",
        "\n",
        "        j_new = y[i]*y[j]*alpha[i] + alpha[j] - y[i]*y[j]*i_new\n",
        "\n",
        "        alpha[i] = i_new\n",
        "        alpha[j] = j_new\n",
        "\n",
        "        i += 1\n",
        "      epoch += 1\n",
        "    return alpha\n",
        "  \n",
        "  # use projected gradient descent to solve quadratic program \n",
        "  # refer to Algorithm 6.5 on page 127\n",
        "  # Q[N,N]: quadratic matrix;  y[N]: training labels (+1 or -1)\n",
        "  def PGD(self, Q, y):\n",
        "    N = Q.shape[0]   # num of training samples\n",
        "    alpha = np.zeros(N)\n",
        "    prev_L = 0.0\n",
        "\n",
        "    for epoch in range(self.max_epochs):\n",
        "      indices = np.random.permutation(N)  #randomly shuffle data indices\n",
        "      for batch_start in range(0, N, self.batch_size):\n",
        "        idx = indices[batch_start:batch_start + self.batch_size] # indices of the current subset\n",
        "        alpha_s = alpha[idx]\n",
        "        y_s = y[idx]\n",
        "\n",
        "        grad_s = Q[idx,:] @ alpha - np.ones(idx.shape[0])\n",
        "        proj_grad_s = grad_s - np.dot(y_s,grad_s)/np.dot(y_s, y_s)*y_s\n",
        "\n",
        "        bound = np.zeros(idx.shape[0])\n",
        "        bound[proj_grad_s < 0] = self.C \n",
        "\n",
        "        eta = np.min(np.abs(alpha_s-bound)/(np.abs(proj_grad_s)+0.001))\n",
        "\n",
        "        alpha[idx] -= min(eta, self.lr) * proj_grad_s\n",
        "\n",
        "      L = 0.5 * alpha.T @ Q @ alpha - np.sum(alpha) # objective function \n",
        "      if (L > prev_L): \n",
        "        if (self.debug>0):\n",
        "          print(f'Early stopping at epoch={epoch}! ({L})')\n",
        "        break\n",
        "      \n",
        "      if (self.debug>1):\n",
        "        print(f'[PGD optimizer] epoch = {epoch}: L = {L:.5f}  (# of support vectors = {(alpha>self.threshold).sum()})')\n",
        "        print(f'                 alpha: max={np.max(alpha)} min={np.min(alpha)} orthogonal constraint={np.dot(alpha,y):.2f}')\n",
        "\n",
        "      prev_L = L\n",
        "\n",
        "    return alpha\n",
        "\n",
        "  # train SVM from training samples\n",
        "  # X[N,d]: input features;  y[N]: output labels (+1 or -1)\n",
        "  def fit(self, X, y):\n",
        "    if(self.kernel != 'linear' and self.kernel != 'poly' and self.kernel != 'rbf'):\n",
        "      print(\"Error: only linear/poly/rbf kernel is supported!\")\n",
        "      return\n",
        "\n",
        "    Q = self.QuadraticMatrix(X, y)\n",
        "\n",
        "    if self.optimizer == \"smo\":\n",
        "      alpha = self.SMO(Q, X, y)\n",
        "    elif self.optimizer == \"pgd\":\n",
        "      alpha = self.PGD(Q, y)\n",
        "\n",
        "    #save support vectors (pruning all data with alpha==0)\n",
        "    self.X_SVs = X[alpha>self.threshold]\n",
        "    self.y_SVs = y[alpha>self.threshold]\n",
        "    self.alpha_SVs = alpha[alpha>self.threshold]\n",
        "\n",
        "    if(self.kernel == 'linear'):\n",
        "      self.w = (self.y_SVs * self.alpha_SVs) @ self.X_SVs\n",
        "\n",
        "    # estimate b\n",
        "    idx = np.nonzero(np.logical_and(self.alpha_SVs>self.threshold,self.alpha_SVs<self.C-self.threshold))\n",
        "    if(len(idx) == 0):\n",
        "      idx = np.nonzero(self.alpha_SVs>self.threshold)\n",
        "    # refer to the formula on page 125 (above Figure 6.11) \n",
        "    b = self.y_SVs[idx] - (self.y_SVs * self.alpha_SVs) @ self.Kernel(self.X_SVs, self.X_SVs[idx])\n",
        "    self.b = np.median(b)\n",
        "    \n",
        "    return \n",
        "\n",
        "  # use SVM from prediction (returns a vector of +1 and -1)\n",
        "  # X[N,d]: input features\n",
        "  def predict(self, X):\n",
        "    if(self.kernel != 'linear' and self.kernel != 'poly' and self.kernel != 'rbf'):\n",
        "      print(\"Error: only linear/poly/rbf kernel is supported!\")\n",
        "      return\n",
        "\n",
        "    if(self.kernel == 'linear'):\n",
        "      y = X @ self.w + self.b \n",
        "    else:\n",
        "      y = (self.y_SVs * self.alpha_SVs) @ self.Kernel(self.X_SVs, X) + self.b \n",
        "\n",
        "    return np.sign(y)\n",
        "\n",
        "      ###############################################################################################################################\n",
        "      ################################################### MODIFIED CODE BELOW #######################################################\n",
        "      ###############################################################################################################################\n",
        "\n",
        "  # prediction function I added (returns a vector of predicted labels)\n",
        "  # used for calculating testing accuracy\n",
        "  def predict_label(self, X):\n",
        "    if(self.kernel != 'linear' and self.kernel != 'poly' and self.kernel != 'rbf'):\n",
        "      print(\"Error: only linear/poly/rbf kernel is supported!\")\n",
        "      return\n",
        "\n",
        "    if(self.kernel == 'linear'):\n",
        "      y = X @ self.w + self.b \n",
        "    else:\n",
        "      y = (self.y_SVs * self.alpha_SVs) @ self.Kernel(self.X_SVs, X) + self.b \n",
        "\n",
        "    return np.vectorize(self.label.get)(np.sign(y))\n"
      ],
      "metadata": {
        "id": "7_G14SH1wkxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Linear SVM with SMO**"
      ],
      "metadata": {
        "id": "0ouaxdouoeop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model to store all 45 binary classifiers\n",
        "model_list = []\n",
        "\n",
        "for i in range(9):\n",
        "  for j in range(i+1, 10):\n",
        "    # prepare data\n",
        "    digit_train_index = np.logical_or(train_label == i, train_label == j)\n",
        "    X_train = train_data[digit_train_index]\n",
        "    y_train = train_label[digit_train_index]\n",
        "    digit_test_index = np.logical_or(test_label == i, test_label == j)\n",
        "    X_test = test_data[digit_test_index]\n",
        "    y_test = test_label[digit_test_index]\n",
        "\n",
        "    # normalize all feature vectors to unit-length\n",
        "    X_train = np.transpose (X_train.T / np.sqrt(np.sum(X_train*X_train, axis=1)))\n",
        "    X_test =  np.transpose (X_test.T  / np.sqrt(np.sum(X_test*X_test, axis=1)))\n",
        "\n",
        "    # convert labels: i => -1, j => +1\n",
        "    CUTOFF = (i+j)/2 # any number between i and j\n",
        "    y_train = np.sign(y_train-CUTOFF)\n",
        "    y_test = np.sign(y_test-CUTOFF)\n",
        "\n",
        "    svm = mySVM(max_epochs=20, lr=0.1, optimizer='smo', C=2, kernel='linear', debug=2, I=i, J=j)\n",
        "    svm.fit(X_train,y_train)\n",
        "\n",
        "    model_list.append(svm)\n",
        "\n",
        "    predict = svm.predict(X_train)\n",
        "    train_acc = np.count_nonzero(np.equal(predict,y_train))/y_train.size\n",
        "    predict = svm.predict(X_test)\n",
        "    test_acc = np.count_nonzero(np.equal(predict,y_test))/y_test.size\n",
        "    print(\"Digits {} and {}:\".format(i, j))\n",
        "    print(f'MY Linear SVM: training accuracy={100*train_acc:.2f}%  test accuracy={100*test_acc:.2f}%')\n",
        "\n",
        "#11 minutes"
      ],
      "metadata": {
        "id": "GxNDTjQjohsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396f4109-c339-4aea-8caa-8b8fc847ea10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digits 0 and 1:\n",
            "MY Linear SVM: training accuracy=96.79%  test accuracy=97.07%\n",
            "Digits 0 and 2:\n",
            "MY Linear SVM: training accuracy=96.84%  test accuracy=97.02%\n",
            "Digits 0 and 3:\n",
            "MY Linear SVM: training accuracy=96.70%  test accuracy=97.89%\n",
            "Digits 0 and 4:\n",
            "MY Linear SVM: training accuracy=98.66%  test accuracy=99.29%\n",
            "Digits 0 and 5:\n",
            "MY Linear SVM: training accuracy=94.33%  test accuracy=94.98%\n",
            "Digits 0 and 6:\n",
            "MY Linear SVM: training accuracy=85.34%  test accuracy=83.95%\n",
            "Digits 0 and 7:\n",
            "MY Linear SVM: training accuracy=97.62%  test accuracy=98.41%\n",
            "Digits 0 and 8:\n",
            "MY Linear SVM: training accuracy=96.68%  test accuracy=97.19%\n",
            "Digits 0 and 9:\n",
            "MY Linear SVM: training accuracy=97.19%  test accuracy=97.34%\n",
            "Digits 1 and 2:\n",
            "MY Linear SVM: training accuracy=95.25%  test accuracy=95.71%\n",
            "Digits 1 and 3:\n",
            "MY Linear SVM: training accuracy=96.78%  test accuracy=97.20%\n",
            "Digits 1 and 4:\n",
            "MY Linear SVM: training accuracy=96.16%  test accuracy=95.94%\n",
            "Digits 1 and 5:\n",
            "MY Linear SVM: training accuracy=94.06%  test accuracy=94.23%\n",
            "Digits 1 and 6:\n",
            "MY Linear SVM: training accuracy=96.39%  test accuracy=96.37%\n",
            "Digits 1 and 7:\n",
            "MY Linear SVM: training accuracy=96.95%  test accuracy=97.04%\n",
            "Digits 1 and 8:\n",
            "MY Linear SVM: training accuracy=94.16%  test accuracy=94.83%\n",
            "Digits 1 and 9:\n",
            "MY Linear SVM: training accuracy=96.59%  test accuracy=96.69%\n",
            "Digits 2 and 3:\n",
            "MY Linear SVM: training accuracy=93.36%  test accuracy=93.73%\n",
            "Digits 2 and 4:\n",
            "MY Linear SVM: training accuracy=97.16%  test accuracy=97.27%\n",
            "Digits 2 and 5:\n",
            "MY Linear SVM: training accuracy=94.74%  test accuracy=95.43%\n",
            "Digits 2 and 6:\n",
            "MY Linear SVM: training accuracy=94.22%  test accuracy=95.33%\n",
            "Digits 2 and 7:\n",
            "MY Linear SVM: training accuracy=96.39%  test accuracy=95.49%\n",
            "Digits 2 and 8:\n",
            "MY Linear SVM: training accuracy=94.43%  test accuracy=94.42%\n",
            "Digits 2 and 9:\n",
            "MY Linear SVM: training accuracy=96.39%  test accuracy=96.96%\n",
            "Digits 3 and 4:\n",
            "MY Linear SVM: training accuracy=97.84%  test accuracy=98.39%\n",
            "Digits 3 and 5:\n",
            "MY Linear SVM: training accuracy=87.46%  test accuracy=88.49%\n",
            "Digits 3 and 6:\n",
            "MY Linear SVM: training accuracy=97.83%  test accuracy=98.12%\n",
            "Digits 3 and 7:\n",
            "MY Linear SVM: training accuracy=96.63%  test accuracy=96.17%\n",
            "Digits 3 and 8:\n",
            "MY Linear SVM: training accuracy=88.81%  test accuracy=89.97%\n",
            "Digits 3 and 9:\n",
            "MY Linear SVM: training accuracy=95.54%  test accuracy=96.24%\n",
            "Digits 4 and 5:\n",
            "MY Linear SVM: training accuracy=95.80%  test accuracy=96.53%\n",
            "Digits 4 and 6:\n",
            "MY Linear SVM: training accuracy=97.28%  test accuracy=96.34%\n",
            "Digits 4 and 7:\n",
            "MY Linear SVM: training accuracy=96.59%  test accuracy=97.11%\n",
            "Digits 4 and 8:\n",
            "MY Linear SVM: training accuracy=96.00%  test accuracy=96.42%\n",
            "Digits 4 and 9:\n",
            "MY Linear SVM: training accuracy=86.27%  test accuracy=86.89%\n",
            "Digits 5 and 6:\n",
            "MY Linear SVM: training accuracy=95.27%  test accuracy=95.19%\n",
            "Digits 5 and 7:\n",
            "MY Linear SVM: training accuracy=96.09%  test accuracy=95.78%\n",
            "Digits 5 and 8:\n",
            "MY Linear SVM: training accuracy=92.70%  test accuracy=93.09%\n",
            "Digits 5 and 9:\n",
            "MY Linear SVM: training accuracy=95.53%  test accuracy=96.27%\n",
            "Digits 6 and 7:\n",
            "MY Linear SVM: training accuracy=98.55%  test accuracy=97.94%\n",
            "Digits 6 and 8:\n",
            "MY Linear SVM: training accuracy=97.31%  test accuracy=96.84%\n",
            "Digits 6 and 9:\n",
            "MY Linear SVM: training accuracy=99.21%  test accuracy=98.93%\n",
            "Digits 7 and 8:\n",
            "MY Linear SVM: training accuracy=96.22%  test accuracy=95.40%\n",
            "Digits 7 and 9:\n",
            "MY Linear SVM: training accuracy=91.40%  test accuracy=91.80%\n",
            "Digits 8 and 9:\n",
            "MY Linear SVM: training accuracy=93.98%  test accuracy=94.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# prepare test data\n",
        "X_test =  np.transpose (test_data.T  / np.sqrt(np.sum(test_data*test_data, axis=1)))\n",
        "y_test = test_label\n",
        "\n",
        "# array to hold prediction\n",
        "result_arr = np.array([np.zeros(y_test.shape)])\n",
        "\n",
        "# compute overall accuracy\n",
        "for model in model_list:\n",
        "  result = model.predict_label(X_test)\n",
        "  result_arr = np.append(result_arr, [np.array(result)], axis=0)\n",
        "\n",
        "#transpose and use the most frequent label in each testing sample as the final label\n",
        "result_arr = (np.delete(result_arr, 0, axis=0)).T\n",
        "prediction = np.array([Counter(sorted(row, reverse=True)).most_common(1)[0][0] for row in result_arr])\n",
        "\n",
        "test_acc = np.count_nonzero(np.equal(y_test, prediction))/y_test.size\n",
        "print(test_acc)"
      ],
      "metadata": {
        "id": "GYlThtoWoiAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da065bb-595f-4ed5-ad3d-3742c11f1857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Linear SVM with PGD**"
      ],
      "metadata": {
        "id": "Op-09KnOoaPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_list = []\n",
        "for i in range(9):\n",
        "  for j in range(i+1, 10):\n",
        "    # prepare data\n",
        "    digit_train_index = np.logical_or(train_label == i, train_label == j)\n",
        "    X_train = train_data[digit_train_index]\n",
        "    y_train = train_label[digit_train_index]\n",
        "    digit_test_index = np.logical_or(test_label == i, test_label == j)\n",
        "    X_test = test_data[digit_test_index]\n",
        "    y_test = test_label[digit_test_index]\n",
        "\n",
        "    # normalize all feature vectors to unit-length\n",
        "    X_train = np.transpose (X_train.T / np.sqrt(np.sum(X_train*X_train, axis=1)))\n",
        "    X_test =  np.transpose (X_test.T  / np.sqrt(np.sum(X_test*X_test, axis=1)))\n",
        "\n",
        "    # convert labels: i => -1, j => +1\n",
        "    CUTOFF = (i+j)/2 # any number between i and j\n",
        "    y_train = np.sign(y_train-CUTOFF)\n",
        "    y_test = np.sign(y_test-CUTOFF)\n",
        "\n",
        "    svm = mySVM(max_epochs=20, lr=0.1, optimizer='pgd', C=4, kernel='linear', I=i, J=j)\n",
        "    svm.fit(X_train,y_train)\n",
        "\n",
        "    model_list.append(svm)\n",
        "\n",
        "    predict = svm.predict(X_train)\n",
        "    train_acc = np.count_nonzero(np.equal(predict,y_train))/y_train.size\n",
        "    predict = svm.predict(X_test)\n",
        "    test_acc = np.count_nonzero(np.equal(predict,y_test))/y_test.size\n",
        "    print(\"Digits {} and {}:\".format(i, j))\n",
        "    print(f'MY Linear SVM: training accuracy={100*train_acc:.2f}%  test accuracy={100*test_acc:.2f}%')\n",
        "\n",
        "# 14 minutes"
      ],
      "metadata": {
        "id": "_1PL2ShZoi8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab63e203-5794-4124-d03a-3f1e8eaca7cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digits 0 and 1:\n",
            "MY Linear SVM: training accuracy=99.90%  test accuracy=99.91%\n",
            "Digits 0 and 2:\n",
            "MY Linear SVM: training accuracy=99.11%  test accuracy=99.30%\n",
            "Digits 0 and 3:\n",
            "MY Linear SVM: training accuracy=99.44%  test accuracy=99.75%\n",
            "Digits 0 and 4:\n",
            "MY Linear SVM: training accuracy=99.71%  test accuracy=99.85%\n",
            "Digits 0 and 5:\n",
            "MY Linear SVM: training accuracy=98.98%  test accuracy=99.09%\n",
            "Digits 0 and 6:\n",
            "MY Linear SVM: training accuracy=99.24%  test accuracy=99.02%\n",
            "Digits 0 and 7:\n",
            "MY Linear SVM: training accuracy=99.74%  test accuracy=99.60%\n",
            "Digits 0 and 8:\n",
            "MY Linear SVM: training accuracy=99.34%  test accuracy=99.44%\n",
            "Digits 0 and 9:\n",
            "MY Linear SVM: training accuracy=99.52%  test accuracy=99.30%\n",
            "Digits 1 and 2:\n",
            "MY Linear SVM: training accuracy=99.24%  test accuracy=99.40%\n",
            "Digits 1 and 3:\n",
            "MY Linear SVM: training accuracy=99.22%  test accuracy=99.63%\n",
            "Digits 1 and 4:\n",
            "MY Linear SVM: training accuracy=99.63%  test accuracy=99.91%\n",
            "Digits 1 and 5:\n",
            "MY Linear SVM: training accuracy=99.63%  test accuracy=99.65%\n",
            "Digits 1 and 6:\n",
            "MY Linear SVM: training accuracy=99.84%  test accuracy=99.62%\n",
            "Digits 1 and 7:\n",
            "MY Linear SVM: training accuracy=99.57%  test accuracy=99.45%\n",
            "Digits 1 and 8:\n",
            "MY Linear SVM: training accuracy=98.45%  test accuracy=99.10%\n",
            "Digits 1 and 9:\n",
            "MY Linear SVM: training accuracy=99.57%  test accuracy=99.58%\n",
            "Digits 2 and 3:\n",
            "MY Linear SVM: training accuracy=97.49%  test accuracy=97.85%\n",
            "Digits 2 and 4:\n",
            "MY Linear SVM: training accuracy=98.73%  test accuracy=98.61%\n",
            "Digits 2 and 5:\n",
            "MY Linear SVM: training accuracy=98.14%  test accuracy=97.97%\n",
            "Digits 2 and 6:\n",
            "MY Linear SVM: training accuracy=98.54%  test accuracy=98.49%\n",
            "Digits 2 and 7:\n",
            "MY Linear SVM: training accuracy=98.78%  test accuracy=97.86%\n",
            "Digits 2 and 8:\n",
            "MY Linear SVM: training accuracy=97.48%  test accuracy=98.06%\n",
            "Digits 2 and 9:\n",
            "MY Linear SVM: training accuracy=98.91%  test accuracy=98.58%\n",
            "Digits 3 and 4:\n",
            "MY Linear SVM: training accuracy=99.58%  test accuracy=99.65%\n",
            "Digits 3 and 5:\n",
            "MY Linear SVM: training accuracy=96.34%  test accuracy=96.32%\n",
            "Digits 3 and 6:\n",
            "MY Linear SVM: training accuracy=99.59%  test accuracy=99.39%\n",
            "Digits 3 and 7:\n",
            "MY Linear SVM: training accuracy=98.82%  test accuracy=98.18%\n",
            "Digits 3 and 8:\n",
            "MY Linear SVM: training accuracy=97.17%  test accuracy=96.77%\n",
            "Digits 3 and 9:\n",
            "MY Linear SVM: training accuracy=98.57%  test accuracy=98.37%\n",
            "Digits 4 and 5:\n",
            "MY Linear SVM: training accuracy=99.17%  test accuracy=99.31%\n",
            "Digits 4 and 6:\n",
            "MY Linear SVM: training accuracy=99.28%  test accuracy=99.07%\n",
            "Digits 4 and 7:\n",
            "MY Linear SVM: training accuracy=99.02%  test accuracy=99.15%\n",
            "Digits 4 and 8:\n",
            "MY Linear SVM: training accuracy=99.40%  test accuracy=99.39%\n",
            "Digits 4 and 9:\n",
            "MY Linear SVM: training accuracy=97.09%  test accuracy=96.94%\n",
            "Digits 5 and 6:\n",
            "MY Linear SVM: training accuracy=98.26%  test accuracy=97.89%\n",
            "Digits 5 and 7:\n",
            "MY Linear SVM: training accuracy=99.47%  test accuracy=99.32%\n",
            "Digits 5 and 8:\n",
            "MY Linear SVM: training accuracy=96.87%  test accuracy=96.41%\n",
            "Digits 5 and 9:\n",
            "MY Linear SVM: training accuracy=98.88%  test accuracy=98.68%\n",
            "Digits 6 and 7:\n",
            "MY Linear SVM: training accuracy=99.90%  test accuracy=99.75%\n",
            "Digits 6 and 8:\n",
            "MY Linear SVM: training accuracy=99.02%  test accuracy=98.96%\n",
            "Digits 6 and 9:\n",
            "MY Linear SVM: training accuracy=99.83%  test accuracy=99.69%\n",
            "Digits 7 and 8:\n",
            "MY Linear SVM: training accuracy=99.31%  test accuracy=98.80%\n",
            "Digits 7 and 9:\n",
            "MY Linear SVM: training accuracy=96.37%  test accuracy=96.42%\n",
            "Digits 8 and 9:\n",
            "MY Linear SVM: training accuracy=98.48%  test accuracy=98.18%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# prepare test data\n",
        "X_test =  np.transpose (test_data.T  / np.sqrt(np.sum(test_data*test_data, axis=1)))\n",
        "y_test = test_label\n",
        "\n",
        "# array to hold prediction\n",
        "result_arr = np.array([np.zeros(y_test.shape)])\n",
        "\n",
        "# compute overall accuracy\n",
        "for model in model_list:\n",
        "  result = model.predict_label(X_test)\n",
        "  result_arr = np.append(result_arr, [np.array(result)], axis=0)\n",
        "\n",
        "#transpose and use the most frequent label in each testing sample as the final label\n",
        "result_arr = (np.delete(result_arr, 0, axis=0)).T\n",
        "prediction = np.array([Counter(sorted(row, reverse=True)).most_common(1)[0][0] for row in result_arr])\n",
        "\n",
        "test_acc = np.count_nonzero(np.equal(y_test, prediction))/y_test.size\n",
        "print(test_acc)"
      ],
      "metadata": {
        "id": "CdeHoQADojeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e4a4d7b-ad8b-4d9d-a658-5635ebcc121a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Nonlinear SVM with SMO**"
      ],
      "metadata": {
        "id": "XwEdHc_coDHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = 2\n",
        "g = 9\n",
        "\n",
        "model_list = []\n",
        "for i in range(9):\n",
        "  for j in range(i+1, 10):\n",
        "    # prepare data\n",
        "    digit_train_index = np.logical_or(train_label == i, train_label == j)\n",
        "    X_train = train_data[digit_train_index]\n",
        "    y_train = train_label[digit_train_index]\n",
        "    digit_test_index = np.logical_or(test_label == i, test_label == j)\n",
        "    X_test = test_data[digit_test_index]\n",
        "    y_test = test_label[digit_test_index]\n",
        "\n",
        "    # normalize all feature vectors to unit-length\n",
        "    X_train = np.transpose (X_train.T / np.sqrt(np.sum(X_train*X_train, axis=1)))\n",
        "    X_test =  np.transpose (X_test.T  / np.sqrt(np.sum(X_test*X_test, axis=1)))\n",
        "\n",
        "    # convert labels: i => -1, j => +1\n",
        "    CUTOFF = (i+j)/2 # any number between i and j\n",
        "    y_train = np.sign(y_train-CUTOFF)\n",
        "    y_test = np.sign(y_test-CUTOFF)\n",
        "\n",
        "    svm = mySVM(max_epochs=20, optimizer='smo', C=c, kernel='rbf', gamma=g, debug=2, I=i, J=j)\n",
        "    svm.fit(X_train,y_train)\n",
        "\n",
        "    model_list.append(svm)\n",
        "\n",
        "    predict = svm.predict(X_train)\n",
        "    train_acc = np.count_nonzero(np.equal(predict,y_train))/y_train.size\n",
        "    predict = svm.predict(X_test)\n",
        "    test_acc = np.count_nonzero(np.equal(predict,y_test))/y_test.size\n",
        "    print(\"Digits {} and {}:\".format(i, j))\n",
        "    print(f'MY RBF SVM (C={c}, gamma={g}): training accuracy={100*train_acc:.2f}%  test accuracy={100*test_acc:.2f}%')\n",
        "\n",
        "# 22 minutes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA_5OZmc5_Go",
        "outputId": "1711432a-0a71-4fbe-cf96-77a7549773b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digits 0 and 1:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=97.09%  test accuracy=97.83%\n",
            "Digits 0 and 2:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.31%  test accuracy=98.21%\n",
            "Digits 0 and 3:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.72%  test accuracy=99.30%\n",
            "Digits 0 and 4:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.89%  test accuracy=99.49%\n",
            "Digits 0 and 5:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.07%  test accuracy=98.34%\n",
            "Digits 0 and 6:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.27%  test accuracy=98.61%\n",
            "Digits 0 and 7:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.89%  test accuracy=99.65%\n",
            "Digits 0 and 8:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.39%  test accuracy=98.21%\n",
            "Digits 0 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.52%  test accuracy=98.69%\n",
            "Digits 1 and 2:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=97.22%  test accuracy=97.88%\n",
            "Digits 1 and 3:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=97.80%  test accuracy=98.23%\n",
            "Digits 1 and 4:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=96.64%  test accuracy=97.45%\n",
            "Digits 1 and 5:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=94.97%  test accuracy=95.86%\n",
            "Digits 1 and 6:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=97.05%  test accuracy=97.75%\n",
            "Digits 1 and 7:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=97.85%  test accuracy=98.15%\n",
            "Digits 1 and 8:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=96.63%  test accuracy=97.49%\n",
            "Digits 1 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=97.16%  test accuracy=97.85%\n",
            "Digits 2 and 3:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.40%  test accuracy=98.09%\n",
            "Digits 2 and 4:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.87%  test accuracy=99.21%\n",
            "Digits 2 and 5:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.93%  test accuracy=98.86%\n",
            "Digits 2 and 6:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.87%  test accuracy=98.69%\n",
            "Digits 2 and 7:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.48%  test accuracy=97.04%\n",
            "Digits 2 and 8:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.25%  test accuracy=97.31%\n",
            "Digits 2 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.80%  test accuracy=98.87%\n",
            "Digits 3 and 4:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.94%  test accuracy=99.70%\n",
            "Digits 3 and 5:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.26%  test accuracy=96.16%\n",
            "Digits 3 and 6:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.88%  test accuracy=99.34%\n",
            "Digits 3 and 7:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.63%  test accuracy=98.09%\n",
            "Digits 3 and 8:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=97.92%  test accuracy=96.47%\n",
            "Digits 3 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=98.72%  test accuracy=97.87%\n",
            "Digits 4 and 5:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.98%  test accuracy=99.04%\n",
            "Digits 4 and 6:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.64%  test accuracy=98.20%\n",
            "Digits 4 and 7:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.74%  test accuracy=98.41%\n",
            "Digits 4 and 8:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.81%  test accuracy=99.18%\n",
            "Digits 4 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=97.46%  test accuracy=94.78%\n",
            "Digits 5 and 6:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.16%  test accuracy=97.24%\n",
            "Digits 5 and 7:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.93%  test accuracy=97.60%\n",
            "Digits 5 and 8:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.49%  test accuracy=97.16%\n",
            "Digits 5 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.19%  test accuracy=97.48%\n",
            "Digits 6 and 7:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.99%  test accuracy=99.85%\n",
            "Digits 6 and 8:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.68%  test accuracy=98.86%\n",
            "Digits 6 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.92%  test accuracy=99.39%\n",
            "Digits 7 and 8:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=99.79%  test accuracy=98.00%\n",
            "Digits 7 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=98.28%  test accuracy=96.86%\n",
            "Digits 8 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=98.66%  test accuracy=97.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# prepare test data\n",
        "X_test =  np.transpose (test_data.T  / np.sqrt(np.sum(test_data*test_data, axis=1)))\n",
        "y_test = test_label\n",
        "\n",
        "# array to hold prediction\n",
        "result_arr = np.array([np.zeros(y_test.shape)])\n",
        "\n",
        "# compute overall accuracy\n",
        "for model in model_list:\n",
        "  result = model.predict_label(X_test)\n",
        "  result_arr = np.append(result_arr, [np.array(result)], axis=0)\n",
        "\n",
        "#transpose and use the most frequent label in each testing sample as the final label\n",
        "result_arr = (np.delete(result_arr, 0, axis=0)).T\n",
        "prediction = np.array([Counter(sorted(row, reverse=True)).most_common(1)[0][0] for row in result_arr])\n",
        "\n",
        "test_acc = np.count_nonzero(np.equal(y_test, prediction))/y_test.size\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTiOKpIb4hk8",
        "outputId": "91e6b455-7841-4f28-f488-9071cbf0a452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Nonlinear SVM with PGD**"
      ],
      "metadata": {
        "id": "AHKw5-qqoKxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = 2\n",
        "g = 9\n",
        "\n",
        "model_list = []\n",
        "for i in range(9):\n",
        "  for j in range(i+1, 10):\n",
        "    # prepare data\n",
        "    digit_train_index = np.logical_or(train_label == i, train_label == j)\n",
        "    X_train = train_data[digit_train_index]\n",
        "    y_train = train_label[digit_train_index]\n",
        "    digit_test_index = np.logical_or(test_label == i, test_label == j)\n",
        "    X_test = test_data[digit_test_index]\n",
        "    y_test = test_label[digit_test_index]\n",
        "\n",
        "    # normalize all feature vectors to unit-length\n",
        "    X_train = np.transpose (X_train.T / np.sqrt(np.sum(X_train*X_train, axis=1)))\n",
        "    X_test =  np.transpose (X_test.T  / np.sqrt(np.sum(X_test*X_test, axis=1)))\n",
        "\n",
        "    # convert labels: i => -1, j => +1\n",
        "    CUTOFF = (i+j)/2 # any number between i and j\n",
        "    y_train = np.sign(y_train-CUTOFF)\n",
        "    y_test = np.sign(y_test-CUTOFF)\n",
        "\n",
        "    svm = mySVM(max_epochs=20, optimizer='pgd', C=c, kernel='rbf', gamma=g, debug=0, I=i, J=j)\n",
        "    svm.fit(X_train,y_train)\n",
        "\n",
        "    model_list.append(svm)\n",
        "\n",
        "    predict = svm.predict(X_train)\n",
        "    train_acc = np.count_nonzero(np.equal(predict,y_train))/y_train.size\n",
        "    predict = svm.predict(X_test)\n",
        "    test_acc = np.count_nonzero(np.equal(predict,y_test))/y_test.size\n",
        "    print(\"Digits {} and {}:\".format(i, j))\n",
        "    print(f'MY RBF SVM (C={c}, gamma={g}): training accuracy={100*train_acc:.2f}%  test accuracy={100*test_acc:.2f}%')\n",
        "\n",
        "#28 minutes"
      ],
      "metadata": {
        "id": "h1koUB1R2cFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396eb5ad-4518-4d27-8be5-c54df0455388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digits 0 and 1:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.10%\n",
            "Digits 0 and 2:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=97.81%\n",
            "Digits 0 and 3:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.49%\n",
            "Digits 0 and 4:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.73%\n",
            "Digits 0 and 5:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=97.54%\n",
            "Digits 0 and 6:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.02%\n",
            "Digits 0 and 7:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.05%\n",
            "Digits 0 and 8:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.52%\n",
            "Digits 0 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.99%\n",
            "Digits 1 and 2:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.85%\n",
            "Digits 1 and 3:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.97%\n",
            "Digits 1 and 4:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.01%\n",
            "Digits 1 and 5:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.87%\n",
            "Digits 1 and 6:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.04%\n",
            "Digits 1 and 7:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.08%\n",
            "Digits 1 and 8:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.00%\n",
            "Digits 1 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.02%\n",
            "Digits 2 and 3:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.17%\n",
            "Digits 2 and 4:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.35%\n",
            "Digits 2 and 5:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.95%\n",
            "Digits 2 and 6:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=97.79%\n",
            "Digits 2 and 7:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=97.09%\n",
            "Digits 2 and 8:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.55%\n",
            "Digits 2 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=97.89%\n",
            "Digits 3 and 4:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.90%\n",
            "Digits 3 and 5:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.53%\n",
            "Digits 3 and 6:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.63%\n",
            "Digits 3 and 7:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.48%\n",
            "Digits 3 and 8:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.94%\n",
            "Digits 3 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.51%\n",
            "Digits 4 and 5:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.41%\n",
            "Digits 4 and 6:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.66%\n",
            "Digits 4 and 7:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.31%\n",
            "Digits 4 and 8:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.80%\n",
            "Digits 4 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=97.94%\n",
            "Digits 5 and 6:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=97.62%\n",
            "Digits 5 and 7:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=97.92%\n",
            "Digits 5 and 8:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.61%\n",
            "Digits 5 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=97.63%\n",
            "Digits 6 and 7:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.60%\n",
            "Digits 6 and 8:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.65%\n",
            "Digits 6 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.69%\n",
            "Digits 7 and 8:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.90%\n",
            "Digits 7 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=99.21%\n",
            "Digits 8 and 9:\n",
            "MY RBF SVM (C=2, gamma=9): training accuracy=100.00%  test accuracy=98.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# prepare test data\n",
        "X_test =  np.transpose (test_data.T  / np.sqrt(np.sum(test_data*test_data, axis=1)))\n",
        "y_test = test_label\n",
        "\n",
        "# array to hold prediction\n",
        "result_arr = np.array([np.zeros(y_test.shape)])\n",
        "\n",
        "# compute overall accuracy\n",
        "for model in model_list:\n",
        "  result = model.predict_label(X_test)\n",
        "  result_arr = np.append(result_arr, [np.array(result)], axis=0)\n",
        "\n",
        "#transpose and use the most frequent label in each testing sample as the final label\n",
        "result_arr = (np.delete(result_arr, 0, axis=0)).T\n",
        "prediction = np.array([Counter(sorted(row, reverse=True)).most_common(1)[0][0] for row in result_arr])\n",
        "\n",
        "test_acc = np.count_nonzero(np.equal(y_test, prediction))/y_test.size\n",
        "print(test_acc)"
      ],
      "metadata": {
        "id": "9ZNpzqnsoSdF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e974dfe-6ad5-4ec7-a67e-67b3e1e5e4ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__Yafp5BJOVA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}